---
title: "Housing Project; predicting  rural house prices based on square footage using linear regression"
author: "Oleksandra Uralska"
date: "2024-01-12"
output: 
  html_document:
    toc: true
    toc_float: 
      collapled: false
      smooth_scroll: false 
    theme: sandstone 
    highlight: tango
    keep_md: true
---
* In this project, we'll develop a simple regression model to predict house prices in the rural area based on their square footage.
* **Goal:** The sole objective of this project is to demonstrate competence in coding with R and navigating RStudio.
* **Data sourse:** We're going to use this [synthetic dataset](https://www.kaggle.com/datasets/muhammadbinimran/housing-price-prediction-data), which includes housing data from 1950 to 2021. I selected this dataset based on positive reviews and its well-organized structure.
* **Data format:** Attributes include *price, square footage, year built, number of bedrooms and bathrooms, and type of neighborhood.*
* **Packages used:** For this project, I use the following packages: *janitor, readxl, dplyr, ggplot2, gridExtra, tibble, ggridges, modelr, broom, knitr, kableExtra*. I load them without displaying the code.
```{r, include = FALSE}
library(janitor)
library(readxl)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(tibble)
library(ggridges)
library(modelr)
library(broom)
library(kableExtra)
library(knitr)
```

## Exploring and preprocessing the data

First, I export the (already downloaded) dataset from Excel.

```{r, results = 'hide'}
excel_file <- "C:\\Learning R\\Data for analysis\\housing_price_dataset.xlsx"
sheet_names <- excel_sheets(excel_file)
print(sheet_names)
my_data <- read_excel(excel_file, sheet = "housing_price_dataset"); str(my_data)
```

  The following tables are the contents of the exported data frame and its summary statistics. 
  
```{r, warning = FALSE, message = FALSE}
kable(head(my_data), caption = paste("The exported housing data in a table format ", "(total number of rows: ", nrow(my_data), ")")) %>%
   kable_styling(full_width = FALSE)
```
```{r}
summary(my_data)
```

  In order to prepare the data for the upcoming analysis, I address several issues that I've noticed while analyzing two tables. Using the *clear_names()* function from the 'janitor' package, I reformatted column names for convenience. I converted the rows of the 'neighbourhood' column into factor variables and removed the column(s) containing negative prices, which likely resulted from errors during data collection. Additionally, I filtered out rows with NA values. 
  
  I prefer to always keep the original dataset as a separate object. This ensures that, in case of errors or mistakes during data manipulations, the original data can be easily reused without the need to retrieve it from an external source. In the code below, a new object named 'data' is created, containing the original dataset with the highlighted issues addressed.
  
```{r}
data <- clean_names(my_data)
data$neighborhood <- as.factor(data$neighborhood)
data <- data %>% 
  filter(data$price > 0)
data <- na.omit(data)

```

## Comparing houses in rural, suburban, and urban areas 

  For this project, we're interested in the rows that correspond to "Rural" factor. Before separating those rows, however, let's study the other two factors in a little more detail. 

```{r}
ggplot(data, aes(neighborhood)) + geom_bar() + ggtitle("The number of houses in each neighbourhood")
```
  The histogram reveals an approximately equal distribution of datapoints across each factor in our dataset.
  
  Our prime interest is the relationship between the house price and its square footage. Let's see how this relationship varies depending on the location of the house.
  
```{r}
ggplot(data, aes(x = square_feet, y = price, 
                 color = neighborhood, shape = neighborhood)) +
  geom_point(alpha = 0.3) 

```
  
  This plot illustrates the multivariate relationship among neighborhood type, house square footage, and price. Its purpose is to answer questions such as whether the houses located in rural areas have higher square footage and lower prices then those in urban areas, etc. The red circles represent the houses located in the rural area, and their respective price and square footage parameters are reflected on the axes. Similarly, the green triangles represent houses in suburban area, and blue squares - those in urban area. 
  
  As we can see, there's no discernible distinction among the three factors, which leads me to a thought that our synthetic dataset may not accurately capture the characteristics of the target population. This lack of distinction could also suggest that the results of our linear regression might be quite similar for all three factors, although making such assumptions with a large dataset entails risks.
  
  In any case, we're now going to focus on the houses located in the rural areas. 
  
## Predicting  rural house prices based on square footage using linear regression

We will conduct a simple linear regression analysis with the house price being a dependent variable, and the square footage - an independent variable. The goal is to determine the intercept, the coefficient (aka slope term), and, of course, to see if our model of predicting the house prices is valid with the help of our to-be-defined test set.
    
We're going to use the following equation:
    
<font size = '3'>"Y = β~0~ + β~1~X + ϵ"</font>

**Where:** 
  
* Y = house price
  
* X = house square footage
  
* β~0~ = intercept
  
* β~1~ = coefficient representing the linear relationship
  
* ϵ = a random error term 

   
### Preparing the data

  To begin with, we'll create a new data table by excluding rows where the factor is not "Rural". In addition, I'll remove the column containing the factor name since it now contains the same character in every row.
  
```{r}
rural1 <- data %>%
  filter(neighborhood == "Rural") %>%
  mutate(neighborhood = factor(neighborhood))
rural1 <- rural1 %>%
  select(-neighborhood)
```

  Exploratory analysis typically involves the use of a training set to discover potential relationships, while a test set is employed to evaluate the effectiveness of the discovered relationships. In the next code chunk, I set a seed to 103 for reproducibility of my analysis, and use the conventional 60/40 split, where I train my model on 60% of my data and test it on the remaining 40%.
  
```{r}
set.seed(103)
sample <- sample(c(TRUE, FALSE), nrow(rural1), replace = T, prob = c(0.6,0.4))
train_data <- rural1[sample, ]
test_data <- rural1[!sample, ]
```

  
  With the help of lm() function, I produce the best-fit linear relationship for the data which minimizes the least-squares criterion.

```{r}
mod <- lm(price ~ square_feet, data = train_data)
tidy_sum <- tidy(mod)
round_tidy_sum <- tidy_sum %>%
  mutate_if(is.numeric, round, digits = 2)
print(round_tidy_sum)
```
  
  The results suggest that our p-value is very close to zero, which means that the values of the intercept and the coefficient are statistically significant. The value of the intercept estimate is not practically meaningful in the context of out data (there’s no house with 0 square feet). The coefficient for square feet, which equals 99.03, is more insightful. It tells us that, on average, each additional square foot leads to an increase of \$99.03 in price. 
  
  With these insights, our model takes the form of:
  
<font size = '3'>"Y = 26124.1 + 99.03X + ϵ"</font>
  
  Next, I access the 95% confidence interval by using confint() function. 
  
```{r}
round(confint(mod), digits = 2)
```
  
  It suggests that, with a 95% confidence, an increase of 1 square feet in the house area raises its price by \$97 – \$101. 
  
### Asessing the model visually

  It is important not only to comprehend quantitative metrics related to our model accuracy, but also to access our model visually for more useful insights.

```{r, warning = FALSE, message = FALSE}
ggplot(rural1, aes(x = square_feet, y = price, )) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = 'lm') +
  geom_smooth(se = FALSE, color = 'red') +
  ggtitle("Vizualization of the linear regression model")

```
  
  On this plot, the linearity of our model (blue line) and a non-linear LOESS model are compared, which helps ensure that our modeling approach aligns with the structure of the training data. Besides, it helps us to decide whether the linear regression is appropriate for our dataset. Clearly, the red and blue lines align very closely, so the linear model adequately captures the underlying relationship between our variables.
  
  Next, we create the residuals vs. fitted values plot, and our primary goal is to find out if the model is linear and if the error terms have constant variance. 
  
```{r, message = FALSE, warning = FALSE}
mod_res <- augment(mod, train_data)
ggplot(mod_res, aes(.fitted, .resid)) +
  geom_ref_line(h = 0) +
  geom_point(alpha = 0.2) +
  geom_smooth(se = FALSE) +
  ggtitle("Residuals vs Fitted")
```
  
  
  Judging by the blue line produced, we can conclude that the assumption of linearity is very accurate. Also, looking at the general shape of our residuals, we don’t see any gaps, a funnel, etc., which suggests that the spread of the residuals is consistent across the range of predicted values, in other words, that the assumption of homoscedasticity is accurate.

  Now, let’s check if we have any outliers and if the residuals are spread equally along ranges of predictors. The first graph is the same as “Residual vs. fitted values” graph, except for the y-axis. 

```{r, warning = FALSE, message = FALSE}
res1 <- ggplot(mod_res, aes(.fitted, .std.resid)) +
  geom_ref_line(h = 0) +
  geom_point(alpha = 0.2) +
  geom_smooth(se = FALSE) +
  ggtitle("Standartized residuals vs \nfitted")
res2 <- ggplot(mod_res, aes(.fitted, sqrt(.std.resid))) +
  geom_ref_line(h = 0) +
  geom_point(alpha = 0.2) +
  geom_smooth(se = FALSE) +
  ggtitle("Scale-location plot")
grid.arrange(res1, res2, nrow = 1)
```


  The first graph shows us deviations of residuals by 1-4 standard deviations (standard deviations are indicated on the y-axis). If we consider a residual that deviates from the predicted value by over 3 standard deviations an outlier, then I can count around *12 outliers*. The plot on the right is needed to to double-check the assumption of homoscedasticity, and better highlight any patterns in the variance. We see a horizontal line and, once again, no clear patter of the residuals, which confirms our claim that the error terms have constant variance.

  As I mentioned, we seem to have some outliers. Let’s determine how much influence these outliers have on our regression line by using Cook's Distance plot and residuals vs. leverage plot. These plots help us find influential data points which, when removed, might significantly alter our regression line.

```{r, warning = FALSE, message = FALSE}
par(mfrow=c(1, 2))
plot(mod, which = 4, id.n = 12)
plot(mod, which = 5, id.n = 12, alpha = 0.2)
```

  We definitely see a few outstanding points which are both to the upper right of our Residuals vs. Leverage plot. However, the extent of their influence is uncertain. To assess this, I generate a new tibble, excluding points with high Cook’s distances, and construct a new residuals plot with the outliers removed. This plot will be displayed alongside our original residuals plot for comparison.

```{r, warning = FALSE, message = FALSE}
trsh <- 4/ length(cooks.distance(mod))
filtr_data <- train_data %>%
  filter(cooks.distance(mod) <= trsh)
resid_plot <- ggplot(mod_res, aes(.fitted, .resid)) +
  geom_ref_line(h = 0) +
  geom_point(alpha = 0.2) +
  geom_smooth(se = FALSE) +
  ggtitle("Residuals vs Fitted, with influential points")
mod_cook <- lm(price ~ square_feet, data = filtr_data) 
mod_res_cook <- augment(mod_cook, filtr_data)
cook_res_plot <- ggplot(mod_res_cook, aes(.fitted, .resid)) +
  geom_ref_line(h = 0) +
  geom_point(alpha = 0.2) +
  geom_smooth(se = FALSE) +
  ggtitle("Residuals vs Fitted, influential points removed")
grid.arrange(resid_plot, cook_res_plot, nrow = 1)
```

  We see that the residuals plot without outliers (the second graph) narrows at the beginning and end but widens in the center, in other words, it has a "funnel" pattern. It is an undesirable situation because our new data violates the assumption of homoscedasticity. Apparently, the outliers played a significant role in stabilizing the variance of our model. Since the assumption of homoscedasticity is crucial in linear regression analysis, I will stick to the original data that includes outliers.

  Now, let’s access the normality of the residuals by building a Q-Q plot. If the points are falling directly on the diagonal line, we can say that the residuals are normally distributed.
  
```{r}
qq_plot <- qqnorm(mod_res$.resid)
qq_plot <- qqline(mod_res$.resid)
```
  
  The normal Q-Q plot looks great, so our data follows normal distribution.

### Accessing model accuracy

  Next, let’s analyze how accurate the model is.  To begin with, we can access RSE (an estimate of a standard deviation of a random error term) using *sigma()* function. 
  
```{r}
sigma(mod)
```
    RSE of our model is 50159.9, which means that the actual house price might deviate from our predicted price by up to \$50,159.9. To find out if this is a significant number, we calculate the percentage error: RSE/mean price. 
  
```{r}
sigma(mod)/mean(rural1$price)
```
  
  It equals to 22.3% which is a fair result. For more certainty, further analysis is needed. Another way to measure the fitness of our model is to find the R^2^ statistic. In our case, R^2^ statistic is the proportion of variation in price that can be explained by the area of the house, in square feet. I calculate it using rsquare() function.

```{r}
rsquare(mod, data = train_data)
```

  It equals to 56%, which means that 56% of variability in price is explained by the variation in square feet of a house. It is a rather moderate result, so I will further test the accuracy of the model by accessing f-statistic value and the p-value (both are found on the bottom of the model summary table).
  
```{r}
summary(mod)
```
  
  F-statistic value is 1.266 * 10^0.4^, while our p-value is 2.210 * 10^-16^. The results of our tests suggest that the accuracy of the produced model is moderate. 
  
### Making predictions 

  Finally, let’s make predictions on the house price based on its square feet. To begin with, I’ll access how the model performs on making predictions against our test data set (40 % of the initial data set). 
  
```{r}
test <- test_data %>% 
  select(-bedrooms, -bathrooms, -year_built) %>%
  add_predictions(mod)
kable(head(test), caption = 
        paste("Actual vs. Predicted price data in the test frame", nrow(test), ")")) %>%
   kable_styling(full_width = FALSE)
```

  The second column provides the actual prices already present in our dataset. The third column shows the price values predicted by our model. If we had no price data available in our test data frame, this is what prices the model would assign to the corresponding square footage.

  To find out if the predictions are accurate, I compute the mean squared prediction error for my test and training data.
  
```{r}
mse_test <- test_data %>% 
  add_predictions(mod) %>%
  summarise(MSE = mean((price - pred)^2))
mse_train <- train_data %>% 
  add_predictions(mod) %>%
  summarise(MSE = mean((price - pred)^2))
cat('MSE for test data: ', mse_test$MSE, 
    '\nMSE for train data: ', mse_train$MSE, '.\n',
    all.equal(mse_test, mse_train))
```
  
  As we can see, the difference is not that significant, which indicates that the model is performing similarly on both the training and test datasets. It is also a sign that the model is not biased towards either the test nor training data, and that is can be described as stable, in a sense that it is not very sensitive to variations in input data

